{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Tutorial 2: Google ADK - Control your LLMs!\n",
    "\n",
    "## LiteLLM, Parameters and Structured Output\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/)\n",
    "\n",
    "### ðŸ“‹ What will you learn in this tutorial?\n",
    "\n",
    "1. **ðŸ”„ Model Flexibility with LiteLLM**\n",
    "   - Use Claude, GPT, Llama and other models in ADK\n",
    "   - Multi-provider configuration\n",
    "\n",
    "2. **âš™ï¸ Fine-tune LLM Behavior**\n",
    "   - Key parameters: temperature, top_p, max_tokens\n",
    "   - Use cases for different configurations\n",
    "\n",
    "3. **ðŸ“Š Structured Output with Pydantic**\n",
    "   - Define data schemas\n",
    "   - Get predictable JSON responses\n",
    "\n",
    "4. **ðŸš€ Practical and Advanced Examples**\n",
    "   - Model comparison\n",
    "   - Complex information extraction\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Tutorial Objective\n",
    "\n",
    "After completing this tutorial, you will be able to:\n",
    "- Integrate any LLM into your ADK agents\n",
    "- Fine-tune model behavior\n",
    "- Get structured and validated responses\n",
    "\n",
    "**Prerequisites:**\n",
    "- Having completed ADK Tutorial 1\n",
    "- API Keys for the models you want to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Initial Setup\n",
    "\n",
    "### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Google ADK with LiteLLM support\n",
    "print(\"ðŸ“¦ Installing Google ADK with LiteLLM...\")\n",
    "!pip install -q google-adk==1.4.2\n",
    "!pip install -q litellm==1.73.0\n",
    "!pip install -qU python-dotenv pydantic\n",
    "\n",
    "print(\"\\nâœ… Installation completed!\")\n",
    "\n",
    "# Check versions\n",
    "import sys\n",
    "print(f\"\\nðŸ Python: {sys.version.split()[0]}\")\n",
    "!pip show google-adk litellm pydantic | grep -E \"Name:|Version:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Keys Configuration\n",
    "\n",
    "For this tutorial, you'll need at least one API Key. You can get them from:\n",
    "- **Google AI Studio**: [https://aistudio.google.com/apikey](https://aistudio.google.com/apikey)\n",
    "- **Anthropic (Claude)**: [https://console.anthropic.com/](https://console.anthropic.com/)\n",
    "- **OpenAI**: [https://platform.openai.com/](https://platform.openai.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Enter them directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "print(\"ðŸ”‘ API Keys Configuration\\n\")\n",
    "print(\"Enter the API Keys you have available (press Enter to skip):\\n\")\n",
    "\n",
    "# Google API Key (required for base examples)\n",
    "if 'GOOGLE_API_KEY' not in os.environ:\n",
    "    google_key = getpass(\"Google API Key (required): \")\n",
    "    if google_key:\n",
    "        os.environ['GOOGLE_API_KEY'] = google_key\n",
    "        os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = 'FALSE'\n",
    "\n",
    "# Anthropic API Key (optional)\n",
    "if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "    anthropic_key = getpass(\"Anthropic API Key (optional): \")\n",
    "    if anthropic_key:\n",
    "        os.environ['ANTHROPIC_API_KEY'] = anthropic_key\n",
    "\n",
    "# OpenAI API Key (optional)\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    openai_key = getpass(\"OpenAI API Key (optional): \")\n",
    "    if openai_key:\n",
    "        os.environ['OPENAI_API_KEY'] = openai_key\n",
    "\n",
    "# Check configuration\n",
    "print(\"\\nðŸ“‹ API Keys Status:\")\n",
    "print(f\"   Google: {'âœ…' if os.environ.get('GOOGLE_API_KEY') else 'âŒ'}\")\n",
    "print(f\"   Anthropic: {'âœ…' if os.environ.get('ANTHROPIC_API_KEY') else 'âŒ'}\")\n",
    "print(f\"   OpenAI: {'âœ…' if os.environ.get('OPENAI_API_KEY') else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Use dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env if it exists\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Part 1: Model Flexibility with LiteLLM\n",
    "\n",
    "### What is LiteLLM?\n",
    "\n",
    "LiteLLM is a library that provides a unified interface for 100+ different language models. It acts as a \"universal translator\" between ADK and various LLM providers.\n",
    "\n",
    "### Advantages of using LiteLLM:\n",
    "\n",
    "- ðŸ§ª **Easy experimentation**: Test different models without changing your code\n",
    "- ðŸ’° **Cost optimization**: Choose the most economical model for each task\n",
    "- ðŸ”“ **No vendor lock-in**: Freedom to change providers\n",
    "- ðŸŽ¯ **Specialized models**: Use the best model for each case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Agents with Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.agents.llm_agent import LlmAgent\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "async def call_agent_async(query: str, runner, user_id, session_id):\n",
    "    \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
    "    print(f\"\\n>>> User query: {query}\")\n",
    "\n",
    "    # Prepare the user message in ADK format\n",
    "    content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "\n",
    "    final_response_text = \"The agent did not produce a final response.\" # Default value\n",
    "\n",
    "    # Key concept: run_async executes the agent's logic and generates events.\n",
    "    # We iterate through events to find the final response.\n",
    "    async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
    "        # You can uncomment the line below to see *all* events during execution\n",
    "        # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
    "\n",
    "        # Key concept: is_final_response() marks the message that concludes the turn.\n",
    "        if event.is_final_response():\n",
    "            if event.content and event.content.parts:\n",
    "                # Assume the text response is in the first part\n",
    "                final_response_text = event.content.parts[0].text\n",
    "            elif event.actions and event.actions.escalate: # Handle possible errors/escalations\n",
    "                final_response_text = f\"The agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "            # Add more validations here if needed (e.g., specific error codes)\n",
    "            break # Stop processing events once final response is found\n",
    "\n",
    "    print(f\"<<< Agent response: {final_response_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Google's Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GEMINI = \"gemini-2.5-flash\"\n",
    "\n",
    "# Example: Defining the basic Agent\n",
    "proverbs_agent = LlmAgent(\n",
    "    model=MODEL_GEMINI,\n",
    "    name=\"proverbs_agent\",\n",
    "    description=\"completes the proverbs that the user starts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_service = InMemorySessionService()\n",
    "\n",
    "APP_NAME = \"test_gemini\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_001\" # Using a fixed ID for simplicity\n",
    "\n",
    "# Create the specific session where the conversation will happen\n",
    "session = await session_service.create_session(app_name=APP_NAME,user_id=USER_ID,session_id=SESSION_ID)\n",
    "# Runner: This is the main component that manages the interaction with the agent.\n",
    "runner_gemini = Runner(agent=proverbs_agent,app_name=APP_NAME,session_service=session_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await call_agent_async(\"Don't look a gift horse...\",\n",
    "                           runner=runner_gemini,\n",
    "                           user_id=USER_ID,\n",
    "                           session_id=SESSION_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.models.lite_llm import LiteLlm\n",
    "openai_model = LiteLlm(\"openai/gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proverbs_agent_openai = LlmAgent(\n",
    "    model=openai_model,\n",
    "    name=\"proverbs_agent\",\n",
    "    description=\"completes the proverbs that the user starts\",\n",
    ")\n",
    "\n",
    "APP_NAME = \"test_openai\"\n",
    "USER_ID = \"user_2\"\n",
    "SESSION_ID = \"session_002\" # Using a fixed ID for simplicity\n",
    "\n",
    "session = await session_service.create_session(app_name=APP_NAME,user_id=USER_ID,session_id=SESSION_ID)\n",
    "runner_openai = Runner(agent=proverbs_agent_openai,app_name=APP_NAME,session_service=session_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await call_agent_async(\"The early bird...\",\n",
    "                           runner=runner_openai,\n",
    "                           user_id=USER_ID,\n",
    "                           session_id=SESSION_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anthropic_model = LiteLlm(\"anthropic/claude-3-5-sonnet-20250116\")\n",
    "proverbs_agent_claude = LlmAgent(\n",
    "    model=anthropic_model,\n",
    "    name=\"proverbs_agent\",\n",
    "    description=\"completes the proverbs that the user starts\",\n",
    ")\n",
    "\n",
    "APP_NAME = \"test_claude\"\n",
    "USER_ID = \"user_3\"\n",
    "SESSION_ID = \"session_003\" # Using a fixed ID for simplicity\n",
    "\n",
    "session = await session_service.create_session(app_name=APP_NAME,user_id=USER_ID,session_id=SESSION_ID)\n",
    "runner_claude = Runner(agent=proverbs_agent_claude,app_name=APP_NAME,session_service=session_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await call_agent_async(\"When in Rome...\",\n",
    "                           runner=runner_claude,\n",
    "                           user_id=USER_ID,\n",
    "                           session_id=SESSION_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Even with Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_model = LiteLlm(\"azure/gpt-4o\")\n",
    "proverbs_agent_azure = LlmAgent(\n",
    "    model=azure_model,\n",
    "    name=\"proverbs_agent\",\n",
    "    description=\"completes the proverbs that the user starts\",\n",
    ")\n",
    "\n",
    "APP_NAME = \"test_azure\"\n",
    "USER_ID = \"user_4\"\n",
    "SESSION_ID = \"session_004\" # Using a fixed ID for simplicity\n",
    "\n",
    "\n",
    "session = await session_service.create_session(app_name=APP_NAME,user_id=USER_ID,session_id=SESSION_ID)\n",
    "runner_azure = Runner(agent=proverbs_agent_azure,app_name=APP_NAME,session_service=session_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await call_agent_async(\"A new broom...\",\n",
    "                           runner=runner_azure,\n",
    "                           user_id=USER_ID,\n",
    "                           session_id=SESSION_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Even local models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_model = LiteLlm(\"ollama/gemma2:2b\")\n",
    "proverbs_agent_ollama = LlmAgent(\n",
    "    model=ollama_model,\n",
    "    name=\"proverbs_agent\",\n",
    "    description=\"completes the proverbs that the user starts\",\n",
    ")\n",
    "\n",
    "APP_NAME = \"test_ollama\"\n",
    "USER_ID = \"user_5\"\n",
    "SESSION_ID = \"session_005\" # Using a fixed ID for simplicity\n",
    "\n",
    "\n",
    "session = await session_service.create_session(app_name=APP_NAME,user_id=USER_ID,session_id=SESSION_ID)\n",
    "runner_ollama = Runner(agent=proverbs_agent_ollama,app_name=APP_NAME,session_service=session_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await call_agent_async(\"A bird in the hand...\",\n",
    "                        runner=runner_ollama,\n",
    "                        user_id=USER_ID,\n",
    "                        session_id=SESSION_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Part 2: Fine-tuning Behavior with Parameters\n",
    "\n",
    "### Key LLM Parameters\n",
    "\n",
    "Parameters allow you to control how models generate text:\n",
    "\n",
    "1. **ðŸŒ¡ï¸ Temperature (0.0 - 2.0)**\n",
    "   - Low (0.0-0.3): Deterministic and conservative responses\n",
    "   - Medium (0.4-0.7): Balance between consistency and creativity\n",
    "   - High (0.8-2.0): Creative and diverse responses\n",
    "\n",
    "2. **ðŸŽ¯ Top-p (0.0 - 1.0)**\n",
    "   - Controls \"nucleus sampling\"\n",
    "   - 0.9 = considers tokens that sum to 90% probability\n",
    "   - Alternative to temperature (use one or the other)\n",
    "\n",
    "3. **ðŸ“ Max Output Tokens**\n",
    "   - Limits response length\n",
    "   - Useful for controlling costs and conciseness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creative Agent (high temperature)\n",
    "creative_agent = LlmAgent(\n",
    "    name=\"CreativeAgent\",\n",
    "    model=azure_model,\n",
    "    description=\"Agent configured for maximum creativity\",\n",
    "    generate_content_config=types.GenerateContentConfig(\n",
    "        temperature= 1.5,          # High creativity\n",
    "        max_output_tokens = 1000,    # Moderate responses\n",
    "        top_k= 40                  # Wide vocabulary\n",
    "     ),\n",
    "    instruction=(\n",
    "        \"You are a creative and imaginative writer.\"\n",
    "        \"Generate original and surprising ideas.\"\n",
    "        \"Use metaphors, analogies and colorful language.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Technical Agent (low temperature)\n",
    "technical_agent = LlmAgent(\n",
    "    name=\"TechnicalAgent\",\n",
    "    model=azure_model,\n",
    "    description=\"Agent configured for technical precision\",\n",
    "    generate_content_config=types.GenerateContentConfig(\n",
    "        temperature= 0.1,          # Very deterministic\n",
    "        max_output_tokens= 150    # Concise responses\n",
    "    ),\n",
    "    instruction=(\n",
    "        \"You are a precise and factual technical expert.\"\n",
    "        \"Provide accurate and verifiable information.\"\n",
    "        \"Avoid speculation and stick to facts.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Balanced Agent (medium configuration)\n",
    "balanced_agent = LlmAgent(\n",
    "    name=\"BalancedAgent\",\n",
    "    model=azure_model,\n",
    "    description=\"Agent with balanced configuration\",\n",
    "    generate_content_config=types.GenerateContentConfig(\n",
    "        temperature= 0.7,          # Balance\n",
    "        max_output_tokens= 300    # Flexible length\n",
    "    ),\n",
    "    instruction=(\n",
    "        \"You are a versatile and adaptable assistant.\"\n",
    "        \"Provide useful and well-structured responses.\"\n",
    "        \"Adapt your style according to context.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Ultra-Concise Agent (limited tokens)\n",
    "concise_agent = LlmAgent(\n",
    "    name=\"ConciseAgent\",\n",
    "    model=azure_model,\n",
    "    description=\"Agent for ultra-brief responses\",\n",
    "    generate_content_config=types.GenerateContentConfig(\n",
    "        temperature= 0.3,\n",
    "        max_output_tokens= 50     # Very limited\n",
    "    ),\n",
    "    instruction=(\n",
    "        \"Respond extremely concisely.\"\n",
    "        \"Maximum 2-3 sentences per response.\"\n",
    "        \"Get straight to the point, no detours.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"ðŸŽ›ï¸ Agents with different parameters created:\")\n",
    "print(\" â€¢ CreativeAgent (temp=1.5)\")\n",
    "print(\" â€¢ TechnicalAgent (temp=0.1)\")\n",
    "print(\" â€¢ BalancedAgent (temp=0.7)\")\n",
    "print(\" â€¢ ConciseAgent (max_tokens=50)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§ª Demonstration: Effect of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"test_creative_agent\"\n",
    "USER_ID = \"user_6\"\n",
    "SESSION_ID = \"session_006\" # Using a fixed ID for simplicity\n",
    "\n",
    "session = await session_service.create_session(app_name=APP_NAME,user_id=USER_ID,session_id=SESSION_ID)\n",
    "runner_creative = Runner(agent=creative_agent,app_name=APP_NAME,session_service=session_service)\n",
    "\n",
    "await call_agent_async(\"Write a poem about the full moon\",\n",
    "                        runner=runner_creative,\n",
    "                        user_id=USER_ID,\n",
    "                        session_id=SESSION_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"test_concise_agent\"\n",
    "USER_ID = \"user_6\"\n",
    "SESSION_ID = \"session_006\" # Using a fixed ID for simplicity\n",
    "\n",
    "\n",
    "session = await session_service.create_session(app_name=APP_NAME,user_id=USER_ID,session_id=SESSION_ID)\n",
    "runner_concise = Runner(agent=concise_agent,app_name=APP_NAME,session_service=session_service)\n",
    "\n",
    "await call_agent_async(\"Write a poem about the full moon\",\n",
    "                        runner=runner_concise,\n",
    "                        user_id=USER_ID,\n",
    "                        session_id=SESSION_ID)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Parameter Usage Guide\n",
    "\n",
    "Recommendations by use case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visual parameter guide\n",
    "import pandas as pd\n",
    "\n",
    "# Create recommendations table\n",
    "parameter_guide = pd.DataFrame([\n",
    "    {\"Use Case\": \"Technical Documentation\", \n",
    "     \"Temperature\": \"0.1-0.3\", \n",
    "     \"Max Tokens\": \"500-1000\", \n",
    "     \"Top-p\": \"0.9-0.95\",\n",
    "     \"Reason\": \"Precision and consistency\"},\n",
    "    \n",
    "    {\"Use Case\": \"Creative Writing\", \n",
    "     \"Temperature\": \"0.8-1.5\", \n",
    "     \"Max Tokens\": \"200-500\", \n",
    "     \"Top-p\": \"0.95-1.0\",\n",
    "     \"Reason\": \"Originality and variety\"},\n",
    "    \n",
    "    {\"Use Case\": \"Customer Service Chatbot\", \n",
    "     \"Temperature\": \"0.5-0.7\", \n",
    "     \"Max Tokens\": \"100-200\", \n",
    "     \"Top-p\": \"0.9\",\n",
    "     \"Reason\": \"Balance and efficiency\"},\n",
    "    \n",
    "    {\"Use Case\": \"Data Analysis\", \n",
    "     \"Temperature\": \"0.1-0.2\", \n",
    "     \"Max Tokens\": \"300-500\", \n",
    "     \"Top-p\": \"0.95\",\n",
    "     \"Reason\": \"Accuracy in results\"},\n",
    "    \n",
    "    {\"Use Case\": \"Brainstorming\", \n",
    "     \"Temperature\": \"1.0-1.8\", \n",
    "     \"Max Tokens\": \"150-300\", \n",
    "     \"Top-p\": \"0.95-1.0\",\n",
    "     \"Reason\": \"Maximum idea diversity\"}\n",
    "])\n",
    "\n",
    "print(\"ðŸ“Š PARAMETER GUIDE BY USE CASE\\n\")\n",
    "print(parameter_guide.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nâš ï¸ Important notes:\")\n",
    "print(\"   â€¢ These are suggested ranges, experiment according to your needs\")\n",
    "print(\"   â€¢ Don't use temperature and top-p simultaneously at extreme values\")\n",
    "print(\"   â€¢ Cost increases with max_tokens, use prudently\")\n",
    "print(\"   â€¢ Some models have specific limits, check documentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Part 3: Structured Output with Pydantic\n",
    "\n",
    "### Why Structured Output?\n",
    "\n",
    "Structured output is crucial when you need to:\n",
    "- ðŸ”§ Integrate AI responses with other systems\n",
    "- ðŸ’¾ Store data in databases\n",
    "- ðŸŽ¯ Guarantee consistent format\n",
    "- âœ… Validate extracted information\n",
    "\n",
    "### Pydantic: The Solution\n",
    "\n",
    "Pydantic allows defining data schemas with:\n",
    "- Python type hints\n",
    "- Automatic validation\n",
    "- JSON serialization\n",
    "- Clear documentation for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pydantic and create data models\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "print(\"ðŸ“š Creating Pydantic models for structured output...\\n\")\n",
    "\n",
    "# Model 1: Product Information\n",
    "class ProductInformation(BaseModel):\n",
    "    \"\"\"Schema for extracting product information\"\"\"\n",
    "    name: str = Field(description=\"Complete product name\")\n",
    "    brand: Optional[str] = Field(None, description=\"Product brand\")\n",
    "    price: Optional[float] = Field(None, description=\"Price in USD\")\n",
    "    features: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of main features\"\n",
    "    )\n",
    "    available: bool = Field(True, description=\"If it's available\")\n",
    "    category: Optional[str] = Field(None, description=\"Product category\")\n",
    "\n",
    "# Model 2: Sentiment Analysis\n",
    "class SentimentAnalysis(BaseModel):\n",
    "    \"\"\"Schema for text sentiment analysis\"\"\"\n",
    "    sentiment: str = Field(\n",
    "        description=\"General sentiment: positive, negative or neutral\"\n",
    "    )\n",
    "    confidence: float = Field(\n",
    "        description=\"Analysis confidence level (0.0 to 1.0)\",\n",
    "        ge=0.0, le=1.0\n",
    "    )\n",
    "    emotions: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Emotions detected in the text\"\n",
    "    )\n",
    "    positive_aspects: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Positive aspects mentioned\"\n",
    "    )\n",
    "    negative_aspects: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Negative aspects mentioned\"\n",
    "    )\n",
    "\n",
    "# Model 3: Event Extraction\n",
    "class Event(BaseModel):\n",
    "    \"\"\"Information about an event\"\"\"\n",
    "    title: str = Field(description=\"Event title\")\n",
    "    date: Optional[str] = Field(None, description=\"Event date (YYYY-MM-DD)\")\n",
    "    time: Optional[str] = Field(None, description=\"Event time (HH:MM)\")\n",
    "    location: Optional[str] = Field(None, description=\"Event location\")\n",
    "    participants: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of participants\"\n",
    "    )\n",
    "    description: Optional[str] = Field(None, description=\"Event description\")\n",
    "\n",
    "class EventList(BaseModel):\n",
    "    \"\"\"List of extracted events\"\"\"\n",
    "    events: List[Event] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of all events found\"\n",
    "    )\n",
    "    total_events: int = Field(\n",
    "        description=\"Total number of events found\"\n",
    "    )\n",
    "\n",
    "print(\"âœ… Pydantic models created:\")\n",
    "print(\"   1. ProductInformation - For extracting product data\")\n",
    "print(\"   2. SentimentAnalysis - For opinion analysis\")\n",
    "print(\"   3. Event/EventList - For extracting event information\")\n",
    "\n",
    "# Usage example\n",
    "print(\"\\nðŸ“‹ Schema example (ProductInformation):\")\n",
    "print(ProductInformation.model_json_schema()['properties'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating agents with structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agents with structured output\n",
    "\n",
    "# Product Extractor Agent\n",
    "product_extractor_agent = LlmAgent(\n",
    "    name=\"ProductExtractor\",\n",
    "    model=\"gemini-2.5-flash\",  # Pro handles structured output better\n",
    "    description=\"Extracts structured product information\",\n",
    "    output_schema=ProductInformation,  # Structured output!\n",
    "    output_key='ProductInformation',  # Output key\n",
    "    generate_content_config=types.GenerateContentConfig(\n",
    "        temperature= 0.1,\n",
    "        max_output_tokens= 300\n",
    "    ),\n",
    "    instruction=(\n",
    "        \"Extract product information from the provided text.\"\n",
    "        \"Follow the defined schema exactly.\"\n",
    "        \"If you don't find some data, use None or empty list as appropriate.\"\n",
    "        \"Be precise with prices and features.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Sentiment Analyzer Agent\n",
    "sentiment_agent = LlmAgent(\n",
    "    name=\"SentimentAnalyzer\",\n",
    "    model=openai_model,\n",
    "    description=\"Analyzes sentiment and emotions in texts\",\n",
    "    output_schema=SentimentAnalysis,  # Structured output!\n",
    "    output_key='SentimentAnalysis',  # Output key\n",
    "    generate_content_config=types.GenerateContentConfig(\n",
    "        temperature= 0.3,\n",
    "        max_output_tokens= 300\n",
    "    ),\n",
    "    instruction=(\n",
    "        \"Analyze the sentiment of the provided text.\"\n",
    "        \"Identify specific emotions present.\"\n",
    "        \"List positive and negative aspects mentioned.\"\n",
    "        \"Assign a confidence level to your analysis (0.0 to 1.0).\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Event Extractor Agent\n",
    "event_agent = LlmAgent(\n",
    "    name=\"EventExtractor\",\n",
    "    model=anthropic_model,\n",
    "    description=\"Extracts event information from text\",\n",
    "    output_schema=EventList,  # Structured output with list!\n",
    "    output_key='EventList',  # Output key\n",
    "    generate_content_config=types.GenerateContentConfig(\n",
    "        temperature= 0.2,\n",
    "        max_output_tokens= 500\n",
    "    ),\n",
    "    instruction=(\n",
    "        \"Extract ALL events mentioned in the text.\"\n",
    "        \"For each event, capture all available information.\"\n",
    "        \"Dates should be in YYYY-MM-DD format.\"\n",
    "        \"Times in HH:MM format.\"\n",
    "        \"If there are multiple events, include them all in the list.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"ðŸŽ¯ Agents with structured output created:\")\n",
    "print(\"   â€¢ ProductExtractor â†’ ProductInformation\")\n",
    "print(\"   â€¢ SentimentAnalyzer â†’ SentimentAnalysis\")\n",
    "print(\"   â€¢ EventExtractor â†’ EventList\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the first product agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"output_1\"\n",
    "USER_ID = \"user_7\"\n",
    "SESSION_ID = \"session_007\" # Using a fixed ID for simplicity\n",
    "\n",
    "product_text = \"\"\"\n",
    "    The new iPhone 15 Pro Max from Apple is now available. \n",
    "    Priced at $1,199, it includes a 48MP camera, 6.7-inch display,\n",
    "    A17 Pro chip and long-lasting battery. Available in titanium.\n",
    "    \"\"\"\n",
    "\n",
    "session = await session_service.create_session(app_name=APP_NAME,user_id=USER_ID,session_id=SESSION_ID)\n",
    "runner_products = Runner(agent=product_extractor_agent,app_name=APP_NAME,session_service=session_service)\n",
    "\n",
    "await call_agent_async(product_text,\n",
    "                        runner=runner_products,\n",
    "                        user_id=USER_ID,\n",
    "                        session_id=SESSION_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the second sentiment agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"output_2\"\n",
    "USER_ID = \"user_7\"\n",
    "SESSION_ID = \"session_007\" # Using a fixed ID for simplicity\n",
    "\n",
    "opinion_text = \"\"\"\n",
    "    I love my new laptop! The speed is incredible and the screen is beautiful.\n",
    "    However, the battery doesn't last as long as I expected and the price was somewhat high.\n",
    "    Overall, I'm satisfied with the purchase.\n",
    "    \"\"\"\n",
    "\n",
    "session = await session_service.create_session(app_name=APP_NAME,user_id=USER_ID,session_id=SESSION_ID)\n",
    "runner_sentiment = Runner(agent=sentiment_agent,app_name=APP_NAME,session_service=session_service)\n",
    "\n",
    "await call_agent_async(opinion_text,\n",
    "                        runner=runner_sentiment,\n",
    "                        user_id=USER_ID,\n",
    "                        session_id=SESSION_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the third event agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "APP_NAME = \"output_3\"\n",
    "USER_ID = \"user_7\"\n",
    "SESSION_ID = \"session_007\" # Using a fixed ID for simplicity\n",
    "\n",
    "events_text = \"\"\"\n",
    "    Reminder: The AI conference will be on March 15, 2024 at 10:00 AM\n",
    "    at the Convention Center. Dr. Smith and Dr. Johnson will participate.\n",
    "    Afterwards, there will be a practical workshop at 2:00 PM at the same location.\n",
    "    \"\"\"\n",
    "\n",
    "session = await session_service.create_session(app_name=APP_NAME,user_id=USER_ID,session_id=SESSION_ID)\n",
    "runner_events = Runner(agent=event_agent,app_name=APP_NAME,session_service=session_service)\n",
    "\n",
    "await call_agent_async(events_text,\n",
    "                        runner=runner_events,\n",
    "                        user_id=USER_ID,\n",
    "                        session_id=SESSION_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ TUTORIAL 2 SUMMARY: LLM CONTROL\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ LiteLLM - Model Flexibility\n",
    "- âœ… Use any model with the `litellm/` prefix\n",
    "- âœ… Support for **100+ different models**\n",
    "- âœ… Easy switching between providers\n",
    "- âœ… **Cost and performance** optimization\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ LLM Parameters - Control\n",
    "- âœ… `temperature`: controls **creativity vs consistency**\n",
    "- âœ… `max_tokens`: limits response **length** and helps control costs\n",
    "- âœ… `top_p`: alternative to `temperature` for sampling\n",
    "- âœ… Configuration tailored to **use case**\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ Structured Output - Predictable Responses\n",
    "- âœ… Use **Pydantic** to define response schemas\n",
    "- âœ… **Automatic validation** of model-generated data\n",
    "- âœ… Easy integration with other systems\n",
    "- âœ… Production of **consistent and typed JSON**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You have completed Google ADK Tutorial 2. Now you have the power to:\n",
    "- Integrate any LLM into your agents\n",
    "- Fine-tune their behavior\n",
    "- Get structured and validated responses\n",
    "\n",
    "**Next step**: Tutorial 3 - Tools  ðŸ› ï¸\n",
    "\n",
    "---\n",
    "\n",
    "**Questions?** Leave them in the video comments or check the official documentation.\n",
    "\n",
    "**Happy coding with ADK!** ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Additional Resources\n",
    "\n",
    "### Useful Links\n",
    "\n",
    "- **ADK Documentation**: [https://google.github.io/adk-docs/](https://google.github.io/adk-docs/)\n",
    "- **LiteLLM Docs**: [https://docs.litellm.ai/](https://docs.litellm.ai/)\n",
    "- **Pydantic Docs**: [https://docs.pydantic.dev/](https://docs.pydantic.dev/)\n",
    "- **Supported Models**: [Complete LiteLLM List](https://docs.litellm.ai/docs/providers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}